{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22022658NguyenTienKhoi/13-Weather-Forecast/blob/main/Deepseek_OCR_(3B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9IFHL2_uGso"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2km5Ov5fuGsv"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlG-LzRTuGsw"
      },
      "source": [
        "\n",
        "Introducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n",
        "\n",
        "Unsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n",
        "\n",
        "[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n",
        "\n",
        "Introducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBEjzpBAuGsw"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aJra1V9uGsx"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n",
        "    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"zerostratos/unsloth_finetune_deepseek-ocr-200\",\n",
        "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code=True,\n",
        "    unsloth_force_compile=True,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use python 3.12\n",
        "# apt update\n",
        "# apt install python3.12 python3.12-venv python3.12-dev -y\n",
        "# python3.12 -m venv ~/py312env\n",
        "# source ~/py312env/bin/activate\n",
        "\n",
        "# pip install addict transformers==4.46.3 tokenizers==0.20.3 PyMuPDF img2pdf einops easydict addict Pillow numpy\n",
        "# python -m pip install --upgrade pip setuptools wheel\n",
        "# pip install torch==2.3.1 'accelerate>=0.26.0'\n",
        "# pip install flash-attn==2.7.3 --no-build-isolation\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "import tempfile\n",
        "import yaml\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "import random\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from PIL import Image, ImageDraw\n",
        "import re\n",
        "from typing import Tuple, Optional\n",
        "from io import StringIO\n",
        "import sys\n",
        "\n",
        "# Bounding box constants\n",
        "BOUNDING_BOX_PATTERN = re.compile(r\"<\\|det\\|>\\[\\[(\\d+),\\s*(\\d+),\\s*(\\d+),\\s*(\\d+)\\]\\]<\\|/det\\|>\")\n",
        "BOUNDING_BOX_COLOR = \"red\"\n",
        "BOUNDING_BOX_WIDTH = 3\n",
        "NORMALIZATION_FACTOR = 1000\n",
        "\n",
        "# Load configuration\n",
        "def load_config():\n",
        "    config_path = \"config.yaml\"\n",
        "    default_config = {\n",
        "        \"uploads_dir\": \"uploads\",\n",
        "        \"rate_limit_log\": \"rate_limit_violations.log\",\n",
        "        \"request_log\": \"requests.log\",\n",
        "        \"share\": False,\n",
        "        \"rate_limit\": {\n",
        "            \"requests_per_window\": 1,\n",
        "            \"window_seconds\": 10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "            # Merge with defaults\n",
        "            for key, value in default_config.items():\n",
        "                if key not in config:\n",
        "                    config[key] = value\n",
        "            return config\n",
        "    return default_config\n",
        "\n",
        "config = load_config()\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    filename=config['request_log'],\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Create uploads directory\n",
        "Path(config['uploads_dir']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Rate limiting\n",
        "rate_limit_tracker = defaultdict(list)\n",
        "\n",
        "def log_rate_limit_violation(ip_address):\n",
        "    \"\"\"Log IPs that exceed rate limits\"\"\"\n",
        "    with open(config['rate_limit_log'], 'a') as f:\n",
        "        f.write(f\"{datetime.now().isoformat()} - IP: {ip_address}\\n\")\n",
        "\n",
        "def check_rate_limit(ip_address):\n",
        "    \"\"\"Check if IP has exceeded rate limit\"\"\"\n",
        "    now = datetime.now()\n",
        "    window = timedelta(seconds=config['rate_limit']['window_seconds'])\n",
        "\n",
        "    # Clean old entries\n",
        "    rate_limit_tracker[ip_address] = [\n",
        "        timestamp for timestamp in rate_limit_tracker[ip_address]\n",
        "        if now - timestamp < window\n",
        "    ]\n",
        "\n",
        "    # Check limit\n",
        "    if len(rate_limit_tracker[ip_address]) >= config['rate_limit']['requests_per_window']:\n",
        "        log_rate_limit_violation(ip_address)\n",
        "        return False\n",
        "\n",
        "    # Add current request\n",
        "    rate_limit_tracker[ip_address].append(now)\n",
        "    return True\n",
        "\n",
        "def generate_request_id():\n",
        "    \"\"\"Generate unique request ID\"\"\"\n",
        "    millis = int(datetime.now().timestamp() * 1000)\n",
        "    random_chars = ''.join(random.choices(string.ascii_lowercase + string.digits, k=7))\n",
        "    return f\"{millis}_{random_chars}\"\n",
        "\n",
        "def save_request_data(request_id, image, prompt, ip_address):\n",
        "    \"\"\"Save uploaded image and prompt to disk\"\"\"\n",
        "    request_dir = Path(config['uploads_dir']) / request_id\n",
        "    request_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Save image\n",
        "    image_path = request_dir / \"uploaded_image.jpg\"\n",
        "    image.save(image_path)\n",
        "\n",
        "    # Save prompt\n",
        "    prompt_path = request_dir / \"prompt.txt\"\n",
        "    with open(prompt_path, 'w') as f:\n",
        "        f.write(prompt)\n",
        "\n",
        "    # Log request\n",
        "    logging.info(f\"Request ID: {request_id} | IP: {ip_address} | Prompt length: {len(prompt)}\")\n",
        "\n",
        "    return request_dir\n",
        "\n",
        "def parse_ocr_output(raw_output: str) -> str:\n",
        "    \"\"\"Parse raw OCR output to remove debug info and format cleanly\"\"\"\n",
        "    lines = raw_output.split('\\n')\n",
        "    parsed_lines = []\n",
        "    in_content = False\n",
        "\n",
        "    # Patterns to skip (debug/metadata)\n",
        "    skip_patterns = [\n",
        "        'BASE:', 'PATCHES:', 'NO PATCHES', 'directly resize',\n",
        "        'image size:', 'valid image tokens:', 'output texts tokens',\n",
        "        'compression ratio:', 'save results:', '====', '===',\n",
        "    ]\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "\n",
        "        # Skip empty lines and debug patterns\n",
        "        if not stripped or any(pattern in line for pattern in skip_patterns):\n",
        "            continue\n",
        "\n",
        "        # Handle ref/det structured data\n",
        "        if '<|ref|>' in line:\n",
        "            # Extract all reference-detection pairs from this line\n",
        "            import re\n",
        "            pattern = r'<\\|ref\\|>(.*?)<\\|/ref\\|>(?:<\\|det\\|>\\[\\[(.*?)\\]\\]<\\|/det\\|>)?'\n",
        "            matches = re.findall(pattern, line)\n",
        "\n",
        "            if matches:\n",
        "                for ref_text, coords in matches:\n",
        "                    if coords:\n",
        "                        # Format with coordinates\n",
        "                        parsed_lines.append(f\"‚Ä¢ **{ref_text}** ‚Üí `[{coords}]`\")\n",
        "                    else:\n",
        "                        # Just the reference text\n",
        "                        parsed_lines.append(ref_text.strip())\n",
        "            continue\n",
        "\n",
        "        # Regular content - add as is\n",
        "        parsed_lines.append(stripped)\n",
        "\n",
        "    result = '\\n'.join(parsed_lines)\n",
        "    return result if result.strip() else raw_output\n",
        "\n",
        "\n",
        "def extract_and_draw_bounding_boxes(text_result: str, original_image: Image.Image) -> Optional[Image.Image]:\n",
        "    \"\"\"\n",
        "    Extract bounding box coordinates from text result and draw them on the image.\n",
        "\n",
        "    Args:\n",
        "        text_result: OCR text result containing bounding box coordinates\n",
        "        original_image: Original PIL image to draw on\n",
        "\n",
        "    Returns:\n",
        "        PIL image with bounding boxes drawn, or None if no coordinates found\n",
        "    \"\"\"\n",
        "    matches = list(BOUNDING_BOX_PATTERN.finditer(text_result))\n",
        "\n",
        "    if not matches:\n",
        "        return None\n",
        "\n",
        "    print(f\"‚úÖ Found {len(matches)} bounding boxes. Drawing on original image.\")\n",
        "\n",
        "    # Create a copy of the original image for drawing\n",
        "    image_with_bboxes = original_image.copy()\n",
        "    draw = ImageDraw.Draw(image_with_bboxes)\n",
        "    w, h = original_image.size\n",
        "\n",
        "    # Pre-calculate scale factors for better performance\n",
        "    w_scale = w / NORMALIZATION_FACTOR\n",
        "    h_scale = h / NORMALIZATION_FACTOR\n",
        "\n",
        "    for match in matches:\n",
        "        # Extract and scale coordinates\n",
        "        coords = tuple(int(c) for c in match.groups())\n",
        "        x1_norm, y1_norm, x2_norm, y2_norm = coords\n",
        "\n",
        "        # Scale normalized coordinates\n",
        "        x1 = int(x1_norm * w_scale)\n",
        "        y1 = int(y1_norm * h_scale)\n",
        "        x2 = int(x2_norm * w_scale)\n",
        "        y2 = int(y2_norm * h_scale)\n",
        "\n",
        "        # Draw rectangle\n",
        "        draw.rectangle([x1, y1, x2, y2], outline=BOUNDING_BOX_COLOR, width=BOUNDING_BOX_WIDTH)\n",
        "\n",
        "    return image_with_bboxes\n",
        "\n",
        "def find_result_image(path: str) -> Optional[Image.Image]:\n",
        "    \"\"\"\n",
        "    Find pre-generated result image in the specified path.\n",
        "\n",
        "    Args:\n",
        "        path: Directory path to search for result image\n",
        "\n",
        "    Returns:\n",
        "        PIL image if found, otherwise None\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(path):\n",
        "        if \"grounding\" in filename or \"result\" in filename:\n",
        "            try:\n",
        "                image_path = os.path.join(path, filename)\n",
        "                return Image.open(image_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error opening result image {filename}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Setup environment and model\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "model_name = 'deepseek-ai/DeepSeek-OCR'\n",
        "\n",
        "'''# Load model once at startup\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\n",
        "    model_name,\n",
        "    _attn_implementation='flash_attention_2',\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    use_safetensors=True\n",
        ")\n",
        "model = model.eval().cuda()'''\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "# Define prompt templates\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"Document to Markdown\": \"<|grounding|>Convert the document to markdown.\",\n",
        "    \"OCR Image\": \"<|grounding|>OCR this image.\",\n",
        "    \"Free OCR (No Layout)\": \"<image>\\nFree OCR.\",\n",
        "    \"Parse Figure\": \"Parse the figure.\",\n",
        "    \"Describe Image\": \"Describe this image in detail.\",\n",
        "    \"Locate Object by Reference\": \"\",\n",
        "    \"Custom\": \"\"\n",
        "}\n",
        "\n",
        "def update_prompt(template_choice):\n",
        "    \"\"\"Update prompt based on template selection\"\"\"\n",
        "    return PROMPT_TEMPLATES[template_choice]\n",
        "\n",
        "def update_ref_text_visibility(template_choice):\n",
        "    \"\"\"Show/hide reference text input and help based on template\"\"\"\n",
        "    if template_choice == \"Locate Object by Reference\":\n",
        "        help_text = \"\"\"\n",
        "**üí° Quick Guide:**\n",
        "- **Reference Text**: Simply type what you want to find (e.g., \"red car\", \"teacher\")\n",
        "- **Prompt field**: Leave empty unless you need advanced custom prompts\n",
        "- Reference Text takes priority if both are filled\n",
        "        \"\"\"\n",
        "        return gr.Textbox(visible=True), gr.Markdown(value=help_text, visible=True)\n",
        "    else:\n",
        "        return gr.Textbox(visible=False), gr.Markdown(value=\"\", visible=False)\n",
        "\n",
        "def process_image(image, model_size, custom_prompt, use_grounding, ref_text, request: gr.Request) -> Tuple[str, Optional[Image.Image]]:\n",
        "    \"\"\"Process image with DeepSeek-OCR\"\"\"\n",
        "\n",
        "    # Get client IP\n",
        "    ip_address = request.client.host\n",
        "\n",
        "    # Check rate limit\n",
        "    if not check_rate_limit(ip_address):\n",
        "        return f\"Rate limit exceeded. Please wait {config['rate_limit']['window_seconds']} seconds.\", None\n",
        "\n",
        "    # Generate request ID and save data\n",
        "    request_id = generate_request_id()\n",
        "    save_request_data(request_id, image, custom_prompt, ip_address)\n",
        "\n",
        "    # Model size configurations\n",
        "    configs = {\n",
        "        \"Tiny\": {\"base_size\": 512, \"image_size\": 512, \"crop_mode\": False},\n",
        "        \"Small\": {\"base_size\": 640, \"image_size\": 640, \"crop_mode\": False},\n",
        "        \"Base\": {\"base_size\": 1024, \"image_size\": 1024, \"crop_mode\": False},\n",
        "        \"Large\": {\"base_size\": 1280, \"image_size\": 1280, \"crop_mode\": False},\n",
        "        \"Gundam\": {\"base_size\": 1024, \"image_size\": 640, \"crop_mode\": True}\n",
        "    }\n",
        "\n",
        "    config_model = configs[model_size]\n",
        "\n",
        "    # Build prompt\n",
        "    if ref_text and ref_text.strip():\n",
        "        # Localization task\n",
        "        prompt = f\"<image>\\nLocate <|ref|>{ref_text.strip()}<|/ref|> in the image.\"\n",
        "    else:\n",
        "        # Regular tasks - add grounding if checkbox is checked\n",
        "        if use_grounding and \"<|grounding|>\" not in custom_prompt:\n",
        "            prompt = f\"<image>\\n<|grounding|>{custom_prompt}\"\n",
        "        else:\n",
        "            prompt = f\"<image>\\n{custom_prompt}\"\n",
        "\n",
        "    # Create temp directory for processing\n",
        "    with tempfile.TemporaryDirectory() as temp_dir:\n",
        "        temp_image_path = os.path.join(temp_dir, \"input_image.jpg\")\n",
        "        image.save(temp_image_path)\n",
        "\n",
        "        # Capture stdout\n",
        "        captured_output = StringIO()\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = captured_output\n",
        "\n",
        "        try:\n",
        "            # Run inference\n",
        "            result = model.infer(\n",
        "                tokenizer,\n",
        "                prompt=prompt,\n",
        "                image_file=temp_image_path,\n",
        "                output_path=temp_dir,\n",
        "                base_size=config_model[\"base_size\"],\n",
        "                image_size=config_model[\"image_size\"],\n",
        "                crop_mode=config_model[\"crop_mode\"],\n",
        "                save_results=True,\n",
        "                test_compress=True\n",
        "            )\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "        # Get captured text\n",
        "        console_output = captured_output.getvalue()\n",
        "        text_result = console_output if console_output else str(result)\n",
        "\n",
        "        # Parse the output for clean display\n",
        "        parsed_result = parse_ocr_output(text_result)\n",
        "\n",
        "        # Try to extract and draw bounding boxes\n",
        "        result_image = extract_and_draw_bounding_boxes(text_result, image)\n",
        "\n",
        "        # If no bounding boxes found in text, try to find pre-generated result image\n",
        "        if result_image is None:\n",
        "            result_image = find_result_image(temp_dir)\n",
        "\n",
        "        return text_result, parsed_result, result_image\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"DeepSeek OCR\") as demo:\n",
        "    gr.Markdown(\"# DeepSeek-OCR Interface\")\n",
        "    gr.Markdown(\"Extract text, convert documents to markdown, or locate objects with bounding boxes\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
        "\n",
        "            model_size = gr.Dropdown(\n",
        "                choices=[\"Tiny\", \"Small\", \"Base\", \"Large\", \"Gundam\"],\n",
        "                value=\"Gundam\",\n",
        "                label=\"Model Size\"\n",
        "            )\n",
        "\n",
        "            prompt_template = gr.Dropdown(\n",
        "                choices=list(PROMPT_TEMPLATES.keys()),\n",
        "                value=\"Document to Markdown\",\n",
        "                label=\"Prompt Template\"\n",
        "            )\n",
        "\n",
        "            ref_text_input = gr.Textbox(\n",
        "                label=\"Reference Text (for localization task)\",\n",
        "                placeholder=\"e.g., teacher, 20-10, a red car...\",\n",
        "                visible=False,\n",
        "                lines=1\n",
        "            )\n",
        "\n",
        "            help_box = gr.Markdown(\n",
        "                \"\",\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            custom_prompt = gr.Textbox(\n",
        "                value=PROMPT_TEMPLATES[\"Document to Markdown\"],\n",
        "                label=\"Prompt (editable)\",\n",
        "                lines=2,\n",
        "                interactive=True\n",
        "            )\n",
        "\n",
        "            use_grounding = gr.Checkbox(\n",
        "                value=True,\n",
        "                label=\"Use Grounding Mode\"\n",
        "            )\n",
        "\n",
        "            submit_btn = gr.Button(\"Run\", variant=\"primary\")\n",
        "\n",
        "        with gr.Column():\n",
        "            show_raw = gr.Checkbox(\n",
        "                value=False,\n",
        "                label=\"Show Raw Output (with debug info)\"\n",
        "            )\n",
        "\n",
        "            parsed_output = gr.Textbox(\n",
        "                label=\"OCR Result (Cleaned)\",\n",
        "                lines=15,\n",
        "                show_copy_button=True,\n",
        "                visible=True\n",
        "            )\n",
        "\n",
        "            raw_output = gr.Textbox(\n",
        "                label=\"OCR Result (Raw)\",\n",
        "                lines=15,\n",
        "                show_copy_button=True,\n",
        "                visible=False\n",
        "            )\n",
        "\n",
        "            output_image = gr.Image(\n",
        "                label=\"Result Image (with bounding boxes if detected)\",\n",
        "                type=\"pil\"\n",
        "            )\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    ### Model Size Guide:\n",
        "    - **Tiny**: 512√ó512 - Fastest, lower accuracy\n",
        "    - **Small**: 640√ó640 - Balanced speed/accuracy\n",
        "    - **Base**: 1024√ó1024 - Good accuracy\n",
        "    - **Large**: 1280√ó1280 - Best accuracy\n",
        "    - **Gundam**: 1024 base + 640 image with crop mode - Optimized for documents\n",
        "\n",
        "    ### Prompt Templates:\n",
        "    - **Document to Markdown**: Converts documents with layout preservation\n",
        "    - **OCR Image**: Standard OCR for any image\n",
        "    - **Free OCR**: Simple text extraction without layout\n",
        "    - **Parse Figure**: Extracts information from charts/diagrams\n",
        "    - **Describe Image**: Detailed image description\n",
        "    - **Locate Object by Reference**: Find specific objects/text (requires reference text input)\n",
        "    - **Custom**: Write your own prompt\n",
        "\n",
        "    ### Bounding Box Support:\n",
        "    - Automatically detects and draws bounding boxes when available in the result\n",
        "    - Particularly useful for \"Locate Object by Reference\" tasks\n",
        "    - Boxes are drawn in red on the original image\n",
        "    \"\"\")\n",
        "\n",
        "    # Toggle between raw and parsed output (client-side)\n",
        "    def toggle_output_display(show_raw_checked):\n",
        "        return gr.Textbox(visible=not show_raw_checked), gr.Textbox(visible=show_raw_checked)\n",
        "\n",
        "    show_raw.change(\n",
        "        fn=toggle_output_display,\n",
        "        inputs=[show_raw],\n",
        "        outputs=[parsed_output, raw_output]\n",
        "    )\n",
        "\n",
        "    # Handler 1: Update the prompt text\n",
        "    prompt_template.change(\n",
        "        fn=update_prompt,\n",
        "        inputs=prompt_template,\n",
        "        outputs=custom_prompt\n",
        "    )\n",
        "\n",
        "    # Handler 2: Update visibility and help\n",
        "    prompt_template.change(\n",
        "        fn=update_ref_text_visibility,\n",
        "        inputs=prompt_template,\n",
        "        outputs=[ref_text_input, help_box]  # Both components here\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_image,\n",
        "        inputs=[image_input, model_size, custom_prompt, use_grounding, ref_text_input],\n",
        "        outputs=[raw_output, parsed_output, output_image]  # Note: raw first, then parsed\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        share=config['share']\n",
        "    )"
      ],
      "metadata": {
        "id": "60_2fUwvv8KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ4kpZTkuGsz"
      },
      "source": [
        "Let's prepare the OCR model to our local first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7c8F28MuGsz"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIf9MjOquGtF"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "\n",
        "  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}