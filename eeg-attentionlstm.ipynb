{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/khinguyntin/eeg-attentionlstm?scriptVersionId=241795726\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"0c434d2a","metadata":{"_cell_guid":"c24145bc-fb93-488b-9858-3b1b6bae82d6","_uuid":"c726ce06-48e2-4c51-99e2-99bc45a7902a","collapsed":false,"execution":{"iopub.execute_input":"2025-05-25T15:32:18.80223Z","iopub.status.busy":"2025-05-25T15:32:18.801949Z","iopub.status.idle":"2025-05-25T15:32:27.375466Z","shell.execute_reply":"2025-05-25T15:32:27.374652Z"},"jupyter":{"outputs_hidden":false},"papermill":{"duration":8.581681,"end_time":"2025-05-25T15:32:27.376927","exception":false,"start_time":"2025-05-25T15:32:18.795246","status":"completed"},"tags":[]},"outputs":[],"source":["import os\n","import mne\n","import numpy as np\n","import pandas as pd\n","\n","from scipy.signal import butter, filtfilt # Keep original filtfilt for the improved func\n","# Removed resample as it wasn't used, can be added back if needed\n","from sklearn.preprocessing import StandardScaler\n","from scipy.stats import skew, kurtosis\n","# Removed pywt as it wasn't used, can be added back if needed\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import warnings\n","\n","# Suppress specific MNE warnings if desired, adjust as needed\n","warnings.filterwarnings(\"ignore\", message=\".*pick_types.*legacy function.*\")\n","warnings.filterwarnings(\"ignore\", message=\".*maximum epoch persistence is not supported.*\") # Common non-critical warning\n","mne.set_log_level('WARNING') # Reduce MNE verbosity"]},{"cell_type":"code","execution_count":2,"id":"cfc91a7a","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:32:27.388721Z","iopub.status.busy":"2025-05-25T15:32:27.388422Z","iopub.status.idle":"2025-05-25T15:32:27.39533Z","shell.execute_reply":"2025-05-25T15:32:27.394842Z"},"papermill":{"duration":0.01343,"end_time":"2025-05-25T15:32:27.396346","exception":false,"start_time":"2025-05-25T15:32:27.382916","status":"completed"},"tags":[]},"outputs":[],"source":["def bandpass_filter_improved(data, lowcut=12.0, highcut=35.0, fs=250, order=5):\n","    '''Apply a bandpass filter to the EEG data efficiently.\n","    Args:\n","        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n","              or (num_channels, num_timesteps)\n","        lowcut: float, lower frequency bound\n","        highcut: float, upper frequency bound\n","        fs: int, sampling frequency\n","        order: int, filter order\n","    Returns:\n","        filtered_data: ndarray of the same shape as input\n","    '''\n","    nyquist = 0.5 * 250\n","    low = lowcut / nyquist\n","    high = highcut / nyquist\n","\n","    # Input validation for frequencies\n","    low = max(0, low) # Ensure low is not negative\n","    high = min(1.0 - 1e-6 , high) # Ensure high is less than 1 (nyquist limit)\n","    \n","    if low >= high:\n","        print(f\"Warning: lowcut({low}Hz) >= highcut({high}Hz). Returning original data.\")\n","        return data\n","\n","    if high >= 1.0 and low <= 0.0: # Effectively no filtering\n","        print(\"Warning: Filter range covers full spectrum. Returning original data.\")\n","        return data\n","    elif low <= 0.0: # Lowpass filter\n","        # print(\"Applying Lowpass Filter\")\n","        b, a = butter(order, high, btype='low')\n","    elif high >= 1.0: # Highpass filter\n","        # print(\"Applying Highpass Filter\")\n","        b, a = butter(order, low, btype='high')\n","    else: # Bandpass filter\n","        # print(\"Applying Bandpass Filter\")\n","        b, a = butter(order, [low, high], btype='band')\n","\n","    # Apply the filter along the last axis (time dimension)\n","    try:\n","        filtered_data = filtfilt(b, a, data, axis=-1, padlen=min(150, data.shape[-1]-1)) # Add padlen\n","    except ValueError as e:\n","        print(f\"Error during filtering: {e}. Data shape: {data.shape}. Filter params: b={b}, a={a}. Returning original data.\")\n","        # Handle short signals that might cause issues with filtfilt's padding\n","        if \"padlen must be less than\" in str(e):\n","             print(\"Signal too short for default padding. Trying minimal padding.\")\n","             try:\n","                 filtered_data = filtfilt(b, a, data, axis=-1, padlen=1)\n","             except Exception as e_inner:\n","                 print(f\"Minimal padding failed: {e_inner}. Returning original data.\")\n","                 return data\n","        else:\n","             return data\n","\n","    return filtered_data"]},{"cell_type":"code","execution_count":3,"id":"c61cf9ee","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:32:27.40551Z","iopub.status.busy":"2025-05-25T15:32:27.405302Z","iopub.status.idle":"2025-05-25T15:32:27.409051Z","shell.execute_reply":"2025-05-25T15:32:27.408509Z"},"papermill":{"duration":0.009531,"end_time":"2025-05-25T15:32:27.41007","exception":false,"start_time":"2025-05-25T15:32:27.400539","status":"completed"},"tags":[]},"outputs":[],"source":["def normalize_eeg_channelwise(data):\n","    '''\n","    Normalize EEG data using z-score normalization per channel across time.\n","    Handles potential flat channels (std=0).\n","    Args:\n","        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n","              or (num_channels, num_timesteps)\n","    Returns:\n","        data_normalized: ndarray of the same shape as input\n","    '''\n","    # Calculate mean and std along the time axis (last axis)\n","    mean = np.mean(data, axis=-1, keepdims=True)\n","    std = np.std(data, axis=-1, keepdims=True)\n","\n","    # Prevent division by zero for channels with no variance\n","    std[std == 0] = 1e-6  # Replace 0 std with a very small number\n","\n","    return (data - mean) / std"]},{"cell_type":"code","execution_count":4,"id":"6ba7133f","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:32:27.419158Z","iopub.status.busy":"2025-05-25T15:32:27.418951Z","iopub.status.idle":"2025-05-25T15:32:27.424648Z","shell.execute_reply":"2025-05-25T15:32:27.424118Z"},"papermill":{"duration":0.011303,"end_time":"2025-05-25T15:32:27.425633","exception":false,"start_time":"2025-05-25T15:32:27.41433","status":"completed"},"tags":[]},"outputs":[],"source":["def segment_eeg_improved(data, segment_length):\n","    '''\n","    Segment EEG data into fixed-length non-overlapping segments efficiently.\n","    Args:\n","        data: ndarray of shape (num_samples, num_channels, num_timesteps)\n","        segment_length: int, length of each segment in samples\n","    Returns:\n","        segmented_data: ndarray of shape (num_total_segments, num_channels, segment_length)\n","                       Returns None if input data is empty or segment_length is invalid/too large.\n","    '''\n","    if data is None or data.size == 0 or segment_length <= 0:\n","        #print(\"Warning: Empty data or invalid segment length provided to segment_eeg.\")\n","        return None\n","\n","    num_samples, num_channels, num_timesteps = data.shape\n","\n","    if segment_length > num_timesteps:\n","        #print(f\"Warning: Segment length ({segment_length}) > total timesteps ({num_timesteps}). Cannot segment.\")\n","        return None # Or return an empty array: np.empty((0, num_channels, segment_length))\n","\n","    # Calculate number of full segments per sample\n","    num_segments_per_sample = num_timesteps // segment_length\n","\n","    if num_segments_per_sample == 0:\n","         #print(f\"Warning: No full segments possible with length {segment_length} for timesteps {num_timesteps}.\")\n","         return np.empty((0, num_channels, segment_length))\n","\n","    # Truncate data to include only full segments\n","    total_timesteps_used = num_segments_per_sample * segment_length\n","    truncated_data = data[:, :, :total_timesteps_used]\n","\n","    # Reshape to separate segments\n","    # Shape becomes (num_samples, num_channels, num_segments_per_sample, segment_length)\n","    try:\n","        segmented_data = truncated_data.reshape(num_samples, num_channels, num_segments_per_sample, segment_length)\n","    except ValueError as e:\n","        print(f\"Error reshaping data during segmentation: {e}\")\n","        print(f\"Input shape: {data.shape}, Truncated shape: {truncated_data.shape}\")\n","        print(f\"Target reshape: {(num_samples, num_channels, num_segments_per_sample, segment_length)}\")\n","        return None\n","\n","\n","    # Transpose and reshape to desired output: (num_total_segments, num_channels, segment_length)\n","    # Transpose: (num_samples, num_segments_per_sample, num_channels, segment_length)\n","    segmented_data = segmented_data.transpose(0, 2, 1, 3)\n","    # Reshape: (num_samples * num_segments_per_sample, num_channels, segment_length)\n","    final_shape = (num_samples * num_segments_per_sample, num_channels, segment_length)\n","    segmented_data = segmented_data.reshape(final_shape)\n","\n","    return segmented_data"]},{"cell_type":"code","execution_count":5,"id":"3a86cb84","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:32:27.43489Z","iopub.status.busy":"2025-05-25T15:32:27.434644Z","iopub.status.idle":"2025-05-25T15:32:27.452062Z","shell.execute_reply":"2025-05-25T15:32:27.451512Z"},"papermill":{"duration":0.023285,"end_time":"2025-05-25T15:32:27.453036","exception":false,"start_time":"2025-05-25T15:32:27.429751","status":"completed"},"tags":[]},"outputs":[],"source":["def processing_data_corrected(X, segment_length, fs): # Added segment_length and fs params\n","    \"\"\" Processes a single file's data (assumed shape [1, channels, timesteps]) \"\"\"\n","    if X is None or X.size == 0:\n","        return None\n","\n","    # 1. Filter data\n","    # Use the improved, vectorized filter function\n","    filtered_data = bandpass_filter_improved(X, fs=fs) # Pass fs\n","\n","    # 2. Normalize data\n","    # Use the improved, vectorized normalizer (channel-wise within the file)\n","    normalized_data = normalize_eeg_channelwise(filtered_data)\n","\n","    # 3. Segment data\n","    # Use the improved, vectorized segmenter\n","    segmented_data = segment_eeg_improved(normalized_data, segment_length=segment_length)\n","\n","    # Return ALL segments generated for this file\n","    return segmented_data\n","\n","\n","def mapping_data_path(input_paths, labeled_ids_set): # Use set for efficiency\n","    \"\"\" Filters input paths to include only those whose participant ID is in the labeled set. \"\"\"\n","    input_paths_filtered = []\n","    for input_path in input_paths:\n","        try:\n","            basename = os.path.basename(input_path)\n","            # Assuming format like 'sub-CCXXXXXX_...' - adjust if different\n","            parts = basename.split('_')\n","            if len(parts) > 1 and parts[0].startswith('sub-'):\n","                 participant_id = parts[0] # e.g., 'sub-CC110033'\n","                 if participant_id in labeled_ids_set:\n","                    input_paths_filtered.append(input_path)\n","            else:\n","                 print(f\"Warning: Could not parse participant ID from filename: {basename}\")\n","        except Exception as e:\n","            print(f\"Error parsing filename {input_path}: {e}\")\n","    return input_paths_filtered\n","\n","\n","def load_data_corrected(data_parents_path, segment_length_seconds=4): # Parameter for segment length\n","    \"\"\" Loads, processes (all segments), and combines data and labels correctly. \"\"\"\n","    print(f\"Starting data loading from: {data_parents_path}\")\n","    print(f\"Using segment length: {segment_length_seconds} seconds\")\n","\n","    input_paths = sorted([os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(data_parents_path)) for f in fn if f.endswith('.fif')])\n","    label_path = os.path.join(data_parents_path, 'filtered_subjects_with_age.tsv') # Use os.path.join\n","\n","    if not os.path.exists(label_path):\n","        print(f\"Error: Label file not found at {label_path}\")\n","        return None, None\n","    if not input_paths:\n","        print(f\"Error: No .fif files found in {data_parents_path}\")\n","        return None, None\n","\n","    try:\n","        label_df = pd.read_csv(label_path, sep='\\\\t')\n","        # Ensure column names are correct (they might have spaces if copied/pasted)\n","        label_df.columns = [col.strip() for col in label_df.columns]\n","        print(f\"Label file loaded. Columns: {label_df.columns.tolist()}\")\n","        if 'participant_id' not in label_df.columns or 'age' not in label_df.columns:\n","             print(\"Error: Label file must contain 'participant_id' and 'age' columns.\")\n","             return None, None\n","\n","        # Create a mapping from participant_id to age for easier lookup\n","        label_map = label_df.set_index('participant_id')['age'].to_dict()\n","        labeled_ids_set = set(label_df['participant_id'].values) # Use set for faster lookup\n","        print(f\"Found {len(labeled_ids_set)} unique participant IDs with labels.\")\n","\n","    except Exception as e:\n","        print(f\"Error reading or processing label file {label_path}: {e}\")\n","        return None, None\n","\n","    input_paths_filtered = mapping_data_path(input_paths, labeled_ids_set)\n","    print(f\"Found {len(input_paths_filtered)} files matching labeled participants.\")\n","\n","    if not input_paths_filtered:\n","        print(\"Error: No files found that match the participant IDs in the label file.\")\n","        return None, None\n","\n","    X_list = []\n","    y_list = []\n","    fs = None # To store sampling frequency (should be consistent)\n","    segment_length_samples = None # Will be set after loading first file\n","    n_channels = None # To store number of channels (should be consistent)\n","\n","    print(f\"Processing {len(input_paths_filtered)} files...\")\n","    for i, input_path in enumerate(tqdm(input_paths_filtered, desc=\"Loading Files\")):\n","        # print(f\"  File {i+1}/{len(input_paths_filtered)}: {os.path.basename(input_path)}\")\n","        try:\n","            raw = mne.io.read_raw_fif(input_path, preload=True) # preload=True needed for get_data\n","\n","            # --- Determine fs, segment length in samples, and n_channels from first file ---\n","            if fs is None:\n","                fs = raw.info['sfreq']\n","                segment_length_samples = int(fs * segment_length_seconds)\n","                print(f\"    Detected fs={fs:.2f} Hz. Using segment length={segment_length_samples} samples ({segment_length_seconds}s).\")\n","                if segment_length_samples == 0:\n","                     print(\"Error: Calculated segment length in samples is 0. Check fs and segment_length_seconds.\")\n","                     return None, None\n","            elif abs(fs - raw.info['sfreq']) > 1000 : # Check for consistency\n","                 print(f\"    WARNING: Inconsistent sampling frequency! Expected ~{fs:.2f}, got {raw.info['sfreq']:.2f}. Skipping file: {os.path.basename(input_path)}\")\n","                 continue\n","\n","            # --- Select Channels ---\n","            # !!! CRITICAL: Changed misc=True to eeg=True !!!\n","            # Adjust if your channels are truly 'misc', but usually EEG channels are 'eeg'\n","            # Exclude 'bads' and common non-EEG channels like EOG, ECG if present\n","            try:\n","                 picks = mne.pick_types(raw.info, misc=True, meg=False, stim=False, eog=False, ecg=False, exclude='bads')\n","                 if len(picks) == 0:\n","                      # Fallback or alternative? Maybe check 'misc' if 'eeg' fails?\n","                      print(f\"    WARNING: No 'eeg' channels found in {os.path.basename(input_path)}. Trying 'misc'.\")\n","                      picks = mne.pick_types(raw.info, misc=True, exclude='bads')\n","                      if len(picks) == 0:\n","                           print(f\"    WARNING: No 'eeg' or 'misc' channels found. Skipping file.\")\n","                           continue\n","\n","                 channels_to_process = raw.copy().pick(picks=picks)\n","                 current_n_channels = len(channels_to_process.ch_names)\n","                 #print(f\"    Selected {current_n_channels} channels.\")\n","\n","            except Exception as pick_error:\n","                 print(f\"    Error picking channels for {os.path.basename(input_path)}: {pick_error}. Skipping file.\")\n","                 continue\n","\n","\n","            if n_channels is None:\n","                n_channels = current_n_channels\n","                print(f\"    Setting expected number of channels to {n_channels}.\")\n","            elif n_channels != current_n_channels:\n","                 print(f\"    WARNING: Inconsistent number of channels! Expected {n_channels}, got {current_n_channels}. Skipping file: {os.path.basename(input_path)}\")\n","                 continue\n","\n","\n","            # --- Get Data ---\n","            data = channels_to_process.get_data() # Shape: (num_channels, num_timesteps)\n","            if data.shape[1] < segment_length_samples:\n","                 print(f\"    WARNING: File duration ({data.shape[1]/fs:.2f}s) is shorter than segment length ({segment_length_seconds}s). Skipping file: {os.path.basename(input_path)}\")\n","                 continue\n","\n","            # Add a sample dimension for processing functions expecting (samples, channels, time)\n","            data_batch = np.array([data]) # Shape: (1, num_channels, num_timesteps)\n","\n","            # --- Process data - this now returns ALL segments for this file ---\n","            # Shape: (num_segments_in_file, num_channels, segment_length_samples)\n","            data_processed_segments = processing_data_corrected(data_batch, segment_length=segment_length_samples, fs=10)\n","\n","\n","            # --- Accumulate Data and Labels ---\n","            if data_processed_segments is not None and data_processed_segments.shape[0] > 0:\n","                num_segments_in_file = data_processed_segments.shape[0]\n","                # print(f\"    Generated {num_segments_in_file} segments.\")\n","\n","                # Get the participant ID from filename to find the label\n","                basename = os.path.basename(input_path)\n","                participant_id = basename.split('_')[0] # Assumes 'sub-XXXXXX_...' format\n","\n","                if participant_id in label_map:\n","                    age_label = label_map[participant_id]\n","                    X_list.append(data_processed_segments)\n","                    # Repeat the label for each segment generated from this file\n","                    y_list.extend([age_label] * num_segments_in_file)\n","                else:\n","                    print(f\"    WARNING: Could not find label for participant {participant_id}. Skipping segments from this file.\")\n","            # else:\n","            #     print(f\"    WARNING: No segments generated after processing for {os.path.basename(input_path)}.\")\n","\n","        except FileNotFoundError:\n","            print(f\"    ERROR: File not found {input_path}\")\n","        except Exception as e:\n","            print(f\"    ERROR processing file {os.path.basename(input_path)}: {e}\")\n","            # Consider adding more specific error handling if needed\n","\n","    # --- Final Assembly ---\n","    if not X_list:\n","        print(\"Error: No data segments were successfully processed and collected.\")\n","        # Determine expected shape even if empty\n","        final_n_channels = n_channels if n_channels is not None else 0\n","        final_seg_len = segment_length_samples if segment_length_samples is not None else 0\n","        return np.empty((0, final_n_channels, final_seg_len)), np.empty((0,))\n","\n","    # Concatenate all segments from all files\n","    try:\n","        X = np.concatenate(X_list, axis=0)\n","        y = np.array(y_list)\n","    except ValueError as e:\n","         print(f\"Error concatenating data segments: {e}\")\n","         # Print shapes of lists items for debugging\n","         #for idx, item in enumerate(X_list):\n","         #    print(f\" Shape of X_list[{idx}]: {item.shape}\")\n","         return None, None # Indicate failure\n","\n","\n","    print(f\"Finished loading. Final shapes: X={X.shape}, y={y.shape}\")\n","\n","    # --- Sanity Check ---\n","    if X.shape[0] != y.shape[0]:\n","         print(f\"CRITICAL WARNING: Mismatch between number of segments ({X.shape[0]}) and number of labels ({y.shape[0]})!\")\n","\n","    return X, y"]},{"cell_type":"code","execution_count":6,"id":"7f48ff4b","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:32:27.461939Z","iopub.status.busy":"2025-05-25T15:32:27.461704Z","iopub.status.idle":"2025-05-25T15:35:30.706956Z","shell.execute_reply":"2025-05-25T15:35:30.704866Z"},"papermill":{"duration":183.269528,"end_time":"2025-05-25T15:35:30.726642","exception":false,"start_time":"2025-05-25T15:32:27.457114","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Starting data loading from: /kaggle/input/eeg-brain/Neural-Science/data\n","Using segment length: 1.5 seconds\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_19/3735731971.py:57: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  label_df = pd.read_csv(label_path, sep='\\\\t')\n"]},{"name":"stdout","output_type":"stream","text":["Label file loaded. Columns: ['participant_id', 'age']\n","Found 160 unique participant IDs with labels.\n","Found 214 files matching labeled participants.\n","Processing 214 files...\n"]},{"name":"stderr","output_type":"stream","text":["Loading Files:   0%|          | 0/214 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["    Detected fs=250.00 Hz. Using segment length=375 samples (1.5s).\n","    Setting expected number of channels to 52.\n"]},{"name":"stderr","output_type":"stream","text":["Loading Files: 100%|██████████| 214/214 [02:50<00:00,  1.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Finished loading. Final shapes: X=(81283, 52, 375), y=(81283,)\n","\\n--- Data Loading Successful ---\n","Loaded X shape: (81283, 52, 375)\n","Loaded y shape: (81283,)\n"]}],"source":["# Define the path and desired segment length\n","data_path = '/kaggle/input/eeg-brain/Neural-Science/data'\n","# <<< CHOOSE YOUR SEGMENT LENGTH HERE >>>\n","segment_duration_seconds = 1.5 # Example: Use 5-second segments\n","\n","X, y = load_data_corrected(data_path, segment_length_seconds=segment_duration_seconds)\n","\n","# Check if loading was successful before proceeding\n","if X is None or y is None or X.size == 0 or y.size == 0:\n","    print(\"\\\\n--- Data Loading Failed. Stopping Execution. ---\")\n","    # You might want to raise an error or handle this appropriately\n","    # For now, just stop the notebook cell execution if in an interactive environment\n","    assert False, \"Data loading failed, cannot continue.\"\n","else:\n","    print(\"\\\\n--- Data Loading Successful ---\")\n","    print(f\"Loaded X shape: {X.shape}\") # Should be (total_segments, num_channels, segment_len_samples)\n","    print(f\"Loaded y shape: {y.shape}\") # Should be (total_segments,)"]},{"cell_type":"code","execution_count":7,"id":"64f83ed8","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:30.758002Z","iopub.status.busy":"2025-05-25T15:35:30.757322Z","iopub.status.idle":"2025-05-25T15:35:33.930371Z","shell.execute_reply":"2025-05-25T15:35:33.929489Z"},"papermill":{"duration":3.190005,"end_time":"2025-05-25T15:35:33.932119","exception":false,"start_time":"2025-05-25T15:35:30.742114","status":"completed"},"tags":[]},"outputs":[],"source":["from sklearn.linear_model import Lasso\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","# Assume you have functions to extract features like PSD\n","# def extract_psd_features(segment_channel_data, fs=250, bands=None):\n","#     # ... returns a 1D array of band powers for that single channel's segment data\n","#     # e.g., [delta_power, theta_power, alpha_power, beta_power, gamma_power]\n","#     if bands is None:\n","#         bands = {'delta': [0.5, 4], 'theta': [4, 8], 'alpha': [8, 13], 'beta': [13, 30], 'gamma': [30, 50]}\n","#     # Placeholder: Replace with actual PSD calculation (e.g., using Welch)\n","#     # For simplicity, let's say it returns 5 features\n","#     return np.random.rand(len(bands))\n","\n","\n","# Assume X_train_tensor, y_train_tensor are your PyTorch tensors\n","# X_train_tensor shape: (num_segments, num_channels, segment_length)\n","# y_train_tensor shape: (num_segments, 1) or (num_segments,)\n","\n","# Convert to NumPy for scikit-learn\n","X_train_np = X\n","y_train_np = y.ravel() # Ensure y is 1D\n","\n","num_segments, num_channels, segment_length = X_train_np.shape\n","#num_psd_bands = 5 # Example\n","\n","# 1. & 2. Extract features and flatten\n","X_train_for_lasso = []\n","for i in range(1000):\n","    segment_features_flat = []\n","    for j in range(num_channels):\n","        channel_segment_data = X_train_np[i, j, :]\n","        # channel_psd = extract_psd_features(channel_segment_data, fs=YOUR_SAMPLING_RATE)\n","        # For demonstration, let's use mean and std as features per channel for simplicity\n","        channel_mean = np.mean(channel_segment_data)\n","        channel_std = np.std(channel_segment_data)\n","        channel_median = np.median(channel_segment_data)\n","        segment_features_flat.extend([channel_mean, channel_std, channel_median]) # 3 features per channel\n","    X_train_for_lasso.append(segment_features_flat)\n","\n","X_train_for_lasso = np.array(X_train_for_lasso)\n","# Shape: (num_segments, num_channels * 2) in this simplified example\n","\n","# Scale features for Lasso\n","scaler = StandardScaler()\n","X_train_scaled_for_lasso = scaler.fit_transform(X_train_for_lasso)\n","\n","# 3. Apply Lasso\n","# Alpha is the regularization strength. You'll need to tune this (e.g., via cross-validation)\n","# Higher alpha = more sparsity (more zero coefficients)"]},{"cell_type":"code","execution_count":8,"id":"5d883501","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:33.962137Z","iopub.status.busy":"2025-05-25T15:35:33.961427Z","iopub.status.idle":"2025-05-25T15:35:35.394525Z","shell.execute_reply":"2025-05-25T15:35:35.39366Z"},"papermill":{"duration":1.447958,"end_time":"2025-05-25T15:35:35.395702","exception":false,"start_time":"2025-05-25T15:35:33.947744","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Optimal alpha found by LassoCV: 0.3425259688240665\n"]}],"source":["from sklearn.linear_model import LassoCV\n","# ... (prepare X_train_scaled_for_lasso, y_train_np) ...\n","lasso_cv = LassoCV(cv=5, random_state=0, max_iter=5000) # cv=5 for 5-fold CV\n","lasso_cv.fit(X_train_scaled_for_lasso, y_train_np[:1000])\n","print(f\"Optimal alpha found by LassoCV: {lasso_cv.alpha_}\")\n","# Use lasso_cv.coef_ for channel importance\n","lasso_coefs = lasso_cv.coef_\n","'''lasso = Lasso(alpha=0.05, max_iter=1000) # Increased max_iter\n","lasso.fit(X_train_scaled_for_lasso, y_train_np[:500])\n","\n","# 4. Interpret Coefficients\n","lasso_coefs = lasso.coef_ # Shape: (num_channels * 2,)'''\n","\n","# Calculate importance per channel (example: sum of absolute coefficients)\n","num_features_per_channel = 3 # In our simplified example (mean, std)\n","channel_importance = []\n","for j in range(num_channels):\n","    start_idx = j * num_features_per_channel\n","    end_idx = start_idx + num_features_per_channel\n","    channel_coefs = lasso_coefs[start_idx:end_idx]\n","    importance = np.sum(np.abs(channel_coefs))\n","    channel_importance.append(importance)\n","\n","# Select top N channels\n","channel_indices_sorted_by_importance = np.argsort(channel_importance)[::-1] # Descending"]},{"cell_type":"code","execution_count":9,"id":"ff62b17e","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:35.423126Z","iopub.status.busy":"2025-05-25T15:35:35.422385Z","iopub.status.idle":"2025-05-25T15:35:35.42804Z","shell.execute_reply":"2025-05-25T15:35:35.42731Z"},"papermill":{"duration":0.020317,"end_time":"2025-05-25T15:35:35.429135","exception":false,"start_time":"2025-05-25T15:35:35.408818","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["array([ 4, 51, 49, 50, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35,\n","       34, 33, 48, 32, 31, 29, 30, 27, 26, 25, 28, 23, 22, 21, 20, 19, 18,\n","       17, 24, 16, 15, 13, 14, 11, 10,  9, 12,  8,  7,  6,  5,  3,  2,  1,\n","        0])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["channel_indices_sorted_by_importance"]},{"cell_type":"code","execution_count":10,"id":"a91417b1","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:35.455496Z","iopub.status.busy":"2025-05-25T15:35:35.455221Z","iopub.status.idle":"2025-05-25T15:35:37.154764Z","shell.execute_reply":"2025-05-25T15:35:37.153948Z"},"papermill":{"duration":1.714369,"end_time":"2025-05-25T15:35:37.156327","exception":false,"start_time":"2025-05-25T15:35:35.441958","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected channel indices: [ 4 51 49 50 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 48]\n"]}],"source":["NUM_CHANNELS_TO_SELECT = 20 # Example\n","selected_channel_indices = channel_indices_sorted_by_importance[:NUM_CHANNELS_TO_SELECT]\n","\n","print(f\"Selected channel indices: {selected_channel_indices}\")\n","X_selected_channels = X[:, selected_channel_indices, :]\n","del X"]},{"cell_type":"code","execution_count":11,"id":"089e9673","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:37.18331Z","iopub.status.busy":"2025-05-25T15:35:37.182596Z","iopub.status.idle":"2025-05-25T15:35:37.18742Z","shell.execute_reply":"2025-05-25T15:35:37.186676Z"},"papermill":{"duration":0.019169,"end_time":"2025-05-25T15:35:37.188513","exception":false,"start_time":"2025-05-25T15:35:37.169344","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["(1000, 156)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["X_train_for_lasso.shape"]},{"cell_type":"code","execution_count":12,"id":"c60b771b","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:37.21559Z","iopub.status.busy":"2025-05-25T15:35:37.215309Z","iopub.status.idle":"2025-05-25T15:35:38.385728Z","shell.execute_reply":"2025-05-25T15:35:38.384838Z"},"papermill":{"duration":1.185548,"end_time":"2025-05-25T15:35:38.387146","exception":false,"start_time":"2025-05-25T15:35:37.201598","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\\n--- Tensors Created ---\n","X_tensor shape: torch.Size([81283, 20, 375])\n","y_tensor shape: torch.Size([81283, 1])\n"]}],"source":["# Ensure data is valid before converting\n","if X_selected_channels is not None and y is not None and X_selected_channels.size > 0 and y.size > 0:\n","    X_tensor = torch.tensor(X_selected_channels, dtype=torch.float32) # Use float32 for PyTorch\n","    y_tensor = torch.tensor(y, dtype=torch.float32) # Use float32 for regression\n","\n","    # Reshape y to match PyTorch requirements for loss function (batch_size, 1)\n","    y_tensor = y_tensor.view(-1, 1)\n","\n","    print(f\"\\\\n--- Tensors Created ---\")\n","    print(f\"X_tensor shape: {X_tensor.shape}\")\n","    print(f\"y_tensor shape: {y_tensor.shape}\")\n","else:\n","    print(\"\\\\n--- Skipping Tensor Conversion due to Loading Errors ---\")\n","    X_tensor, y_tensor = None, None # Ensure they are None if loading failed\n","del X_selected_channels\n","del y"]},{"cell_type":"code","execution_count":13,"id":"4284fc91","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:38.414464Z","iopub.status.busy":"2025-05-25T15:35:38.413895Z","iopub.status.idle":"2025-05-25T15:35:40.094587Z","shell.execute_reply":"2025-05-25T15:35:40.093768Z"},"papermill":{"duration":1.695834,"end_time":"2025-05-25T15:35:40.096088","exception":false,"start_time":"2025-05-25T15:35:38.400254","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\\n--- DataLoaders Created ---\n","Train samples: 56899, Test samples: 24384\n","Train batches: 7112, Test batches: 3048\n"]}],"source":["# Check if tensors are valid before splitting\n","if X_tensor is not None and y_tensor is not None:\n","    # !!! IMPORTANT NOTE ON SPLITTING !!!\n","    # This basic train_test_split shuffles segments randomly.\n","    # Segments from the SAME SUBJECT might end up in BOTH train and test sets.\n","    # This leads to data leakage and overly optimistic results.\n","    # A better approach is SUBJECT-AWARE splitting: Split participant IDs first,\n","    # then collect all segments belonging to train/test subjects.\n","    # For now, we proceed with the simple split as in the original code.\n","\n","    test_fraction = 0.3 # Use a fraction for robustness if total segments vary\n","    test_size_count = int(len(X_tensor) * test_fraction)\n","    # Ensure test_size is at least 1 if dataset is very small, adjust if needed\n","    test_size_count = max(1, test_size_count) if len(X_tensor) > 0 else 0\n","\n","    if test_size_count > 0 and test_size_count < len(X_tensor):\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            X_tensor, y_tensor, test_size=test_size_count, random_state=42, shuffle=True\n","        )\n","\n","        # Create DataLoader for batch processing\n","        train_dataset = TensorDataset(X_train, y_train)\n","        test_dataset = TensorDataset(X_test, y_test)\n","\n","        batch_size = 8 # Adjusted batch size, tune as needed\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True) # drop_last can help with stability\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","        print(\"\\\\n--- DataLoaders Created ---\")\n","        print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n","        print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")\n","    else:\n","        print(\"\\\\n--- Not enough data to perform train/test split ---\")\n","        train_loader, test_loader = None, None\n","else:\n","    print(\"\\\\n--- Skipping Train/Test Split due to Loading Errors ---\")\n","    train_loader, test_loader = None, None\n","del X_tensor\n","del y_tensor"]},{"cell_type":"code","execution_count":14,"id":"b4a49cae","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:40.123939Z","iopub.status.busy":"2025-05-25T15:35:40.123383Z","iopub.status.idle":"2025-05-25T15:35:40.193939Z","shell.execute_reply":"2025-05-25T15:35:40.193142Z"},"papermill":{"duration":0.085668,"end_time":"2025-05-25T15:35:40.195146","exception":false,"start_time":"2025-05-25T15:35:40.109478","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Baseline MAE (predicting mean age): 15.15984058380127\n"]}],"source":["mean_age_train = torch.mean(y_train)\n","baseline_predictions = torch.full_like(y_test, mean_age_train) # y_val_numpy are true ages in validation\n","baseline_mae = torch.mean(abs(y_test - baseline_predictions))\n","print(f\"Baseline MAE (predicting mean age): {baseline_mae}\")"]},{"cell_type":"code","execution_count":15,"id":"0cffb2c8","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:40.223058Z","iopub.status.busy":"2025-05-25T15:35:40.222764Z","iopub.status.idle":"2025-05-25T15:35:40.231405Z","shell.execute_reply":"2025-05-25T15:35:40.230834Z"},"papermill":{"duration":0.024302,"end_time":"2025-05-25T15:35:40.232512","exception":false,"start_time":"2025-05-25T15:35:40.20821","status":"completed"},"tags":[]},"outputs":[],"source":["class AttentionLSTMModel(nn.Module):\n","    def __init__(self, num_channels, embedding_dim=128, lstm_hidden=64, nhead=1, dim_ff=256, dropout=0.3):\n","        super(AttentionLSTMModel, self).__init__()\n","\n","        self.embedding_dim = embedding_dim\n","        self.input_proj = nn.Linear(num_channels, self.embedding_dim) # Project channels to embedding dim\n","        self.pos_encoder = nn.Identity() # Placeholder: Consider adding positional encoding for transformer\n","\n","        # Attention Block (Transformer Encoder Layer)\n","        # Ensure embedding_dim is divisible by nhead\n","        if embedding_dim % nhead != 0:\n","            # Find nearest valid nhead\n","            valid_nheads = [h for h in range(1, embedding_dim + 1) if embedding_dim % h == 0]\n","            nhead = valid_nheads[-1] if valid_nheads else 1 # Default to 1 if no divisors found\n","            print(f\"Warning: embedding_dim ({embedding_dim}) not divisible by nhead (8). Adjusted nhead to {nhead}.\")\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=self.embedding_dim,\n","            nhead=nhead,\n","            dim_feedforward=dim_ff,\n","            dropout=dropout, # Apply dropout within transformer\n","            batch_first=True,\n","            activation='relu' # Common activation\n","        )\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2) # Single layer transformer\n","\n","        # LSTM Layers - Using nn.LSTM is generally preferred over nn.RNN\n","        self.lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=lstm_hidden, num_layers=1, bidirectional = True,batch_first=True, dropout=dropout if 1 > 1 else 0) # Dropout applies between layers if num_layers > 1\n","        # A second LSTM layer might be too much, consider removing or reducing hidden size\n","        # self.lstm2 = nn.LSTM(input_size=lstm_hidden, hidden_size=lstm_hidden // 2, num_layers=1, batch_first=True, dropout=dropout if 1 > 1 else 0)\n","\n","        # Classifier Head\n","        self.fc = nn.Sequential(\n","            nn.LayerNorm(lstm_hidden * 2), # Add LayerNorm before activation\n","            nn.ReLU(),\n","            nn.Dropout(dropout), # Use consistent dropout rate\n","            nn.Linear(lstm_hidden * 2, lstm_hidden // 2), # Add another layer\n","            nn.LayerNorm(lstm_hidden // 2),\n","            nn.ReLU(),\n","            nn.Dropout(dropout),\n","            nn.Linear(lstm_hidden // 2, 1) # Final output\n","        )\n","\n","    def forward(self, x):\n","        # x shape: (batch_size, channels, timesteps)\n","        # print(\"Input:\", x.shape)\n","        x = x.permute(0, 2, 1)  # → (batch_size, timesteps, channels)\n","        # print(\"Permuted:\", x.shape)\n","        x = self.input_proj(x)  # → (batch_size, timesteps, embedding_dim)\n","        # print(\"Projected:\", x.shape)\n","\n","        # Add positional encoding if using it\n","        x = self.pos_encoder(x)\n","\n","        # Apply Transformer Encoder\n","        x = self.transformer_encoder(x) # → (batch_size, timesteps, embedding_dim)\n","        # print(\"After Transformer:\", x.shape)\n","\n","        # Apply LSTM\n","        # LSTM output: output_seq, (h_n, c_n)\n","        # output_seq shape: (batch_size, timesteps, hidden_size)\n","        # h_n shape: (num_layers, batch_size, hidden_size)\n","        lstm_out, (h_n, _) = self.lstm(x)\n","        #print(\"After LSTM1 output:\", lstm_out.shape)\n","        #print(\"After LSTM1 hidden:\", h_n.shape)\n","\n","        # Optional second LSTM\n","        # lstm_out, (h_n, _) = self.lstm2(lstm_out)\n","        # print(\"After LSTM2 output:\", lstm_out.shape)\n","        # print(\"After LSTM2 hidden:\", h_n.shape)\n","\n","\n","        # Get the output of the last time step from the last LSTM layer\n","        # h_n is shape (num_layers, batch_size, hidden_size), we want the last layer's hidden state\n","        #last_hidden_state = h_n[-1, :, :] # → (batch_size, hidden_size)\n","        # Concatenate them along the feature dimension\n","        last_hidden_state_forward = h_n[0, :, :]  # Shape: (batch_size, hidden_size)\n","        last_hidden_state_backward = h_n[1, :, :] # Shape: (batch_size, hidden_size)\n","\n","        # Concatenate:\n","        last_hidden_state = torch.cat((last_hidden_state_forward, last_hidden_state_backward), dim=1)\n","        #print(\"Last Hidden State:\", last_hidden_state.shape)\n","\n","        # Alternatively, use the last time step of the output sequence:\n","        # last_time_step_output = lstm_out[:, -1, :] # -> (batch_size, hidden_size)\n","\n","        x = self.fc(last_hidden_state) # Feed the final hidden state to the classifier\n","        # print(\"Final Output:\", x.shape)\n","        return x"]},{"cell_type":"code","execution_count":16,"id":"40579498","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:40.25977Z","iopub.status.busy":"2025-05-25T15:35:40.259197Z","iopub.status.idle":"2025-05-25T15:35:46.754378Z","shell.execute_reply":"2025-05-25T15:35:46.753158Z"},"papermill":{"duration":6.510101,"end_time":"2025-05-25T15:35:46.755656","exception":false,"start_time":"2025-05-25T15:35:40.245555","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","\\n--- Model Initialized ---\n","AttentionLSTMModel(\n","  (input_proj): Linear(in_features=20, out_features=128, bias=True)\n","  (pos_encoder): Identity()\n","  (transformer_encoder): TransformerEncoder(\n","    (layers): ModuleList(\n","      (0-1): 2 x TransformerEncoderLayer(\n","        (self_attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n","        )\n","        (linear1): Linear(in_features=128, out_features=256, bias=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (linear2): Linear(in_features=256, out_features=128, bias=True)\n","        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n","        (dropout1): Dropout(p=0.1, inplace=False)\n","        (dropout2): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","  )\n","  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n","  (fc): Sequential(\n","    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    (1): ReLU()\n","    (2): Dropout(p=0.1, inplace=False)\n","    (3): Linear(in_features=256, out_features=64, bias=True)\n","    (4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","    (5): ReLU()\n","    (6): Dropout(p=0.1, inplace=False)\n","    (7): Linear(in_features=64, out_features=1, bias=True)\n","  )\n",")\n","Total Trainable Parameters: 548,993\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}],"source":["from transformers import get_cosine_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup\n","# Ensure data loaders are valid before initializing model\n","if train_loader is not None:\n","    # Get input shape from the loaded data\n","    # X_tensor shape: (total_segments, num_channels, segment_len_samples)\n","    input_channels = 20\n","    # input_timesteps = X_tensor.shape[2] # Timesteps not directly needed by this model init\n","\n","    # --- Hyperparameters ---\n","    EMBEDDING_DIM = 128 # Dimension after initial projection\n","    LSTM_HIDDEN = 128   # LSTM hidden units\n","    NHEAD = 2         # Transformer heads (must divide EMBEDDING_DIM)\n","    DIM_FF = 256       # Transformer feedforward dim\n","    DROPOUT = 0.1      # Dropout rate\n","    LEARNING_RATE = 5e-4 # Learning rate\n","    WEIGHT_DECAY = 1e-5  # Weight decay for regularization\n","\n","    model = AttentionLSTMModel(\n","        num_channels=input_channels,\n","        embedding_dim=EMBEDDING_DIM,\n","        lstm_hidden=LSTM_HIDDEN,\n","        nhead=NHEAD,\n","        dim_ff=DIM_FF,\n","        dropout=DROPOUT\n","    )\n","\n","    # Move model to GPU if available\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    print(f\"Using device: {device}\")\n","\n","    criterion = nn.L1Loss() # MAE Loss for age regression\n","    #criterion = nn.MSELoss() # Option: Mean Squared Error\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # AdamW is often preferred\n","    num_training_steps = 100 * len(train_loader)\n","    num_warmup_steps = 100\n","    #int(0.01 * num_training_steps)  # 10% warmup\n","\n","    '''scheduler = get_polynomial_decay_schedule_with_warmup(\n","                optimizer, \n","                num_warmup_steps=num_warmup_steps,\n","                num_training_steps=num_training_steps\n","    )'''\n","    '''get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps\n","    )'''\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5, verbose=True, min_lr=1e-6) # More patience, lower factor\n","\n","    # Print model summary\n","    print(\"\\\\n--- Model Initialized ---\")\n","    print(model)\n","    # Count parameters\n","    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Total Trainable Parameters: {total_params:,}\")\n","\n","else:\n","    print(\"\\\\n--- Skipping Model Initialization due to Loading/Splitting Errors ---\")\n","    model = None\n","    criterion = None\n","    optimizer = None\n","    scheduler = None\n","    device = \"cpu\" # Default to CPU if no model/GPU"]},{"cell_type":"code","execution_count":17,"id":"eb045161","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:46.783487Z","iopub.status.busy":"2025-05-25T15:35:46.782632Z","iopub.status.idle":"2025-05-25T15:35:46.791296Z","shell.execute_reply":"2025-05-25T15:35:46.790524Z"},"papermill":{"duration":0.023434,"end_time":"2025-05-25T15:35:46.792506","exception":false,"start_time":"2025-05-25T15:35:46.769072","status":"completed"},"tags":[]},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, path_to_save, patience=10, delta=0.001, restore_best_weights=True, verbose=True):\n","        \"\"\"\n","        Args:\n","            path_to_save (str): Path to save the best model state.\n","            patience (int): How many epochs to wait after last time validation loss improved.\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","            restore_best_weights (bool): Whether to restore model weights from the epoch with the best value of the monitored quantity.\n","            verbose (bool): If True, prints a message for each validation loss improvement.\n","        \"\"\"\n","        self.patience = patience\n","        self.delta = delta\n","        self.restore_best_weights = restore_best_weights\n","        self.path_to_save = path_to_save\n","        self.verbose = verbose\n","\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.best_val_loss = np.Inf\n","        self.best_epoch = 0\n","        self.best_weights = None\n","\n","\n","    def __call__(self, val_loss, epoch, model):\n","        score = -val_loss # We want to maximize the negative loss (minimize loss)\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.best_val_loss = val_loss\n","            self.best_epoch = epoch\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            if self.verbose:\n","                 print(f'EarlyStopping counter: {self.counter} out of {self.patience} (Best val_loss: {self.best_val_loss:.4f} at epoch {self.best_epoch})')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","                print(f\"Early stopping triggered after {epoch} epochs.\")\n","        else:\n","            # Improvement detected\n","            self.best_score = score\n","            self.best_val_loss = val_loss\n","            self.best_epoch = epoch\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0 # Reset counter\n","\n","        return self.early_stop\n","\n","    def save_checkpoint(self, val_loss, model):\n","        '''Saves model when validation loss decreases.'''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.best_val_loss:.4f} --> {val_loss:.4f}). Saving model to {self.path_to_save} ...')\n","        # Save the current best weights\n","        self.best_weights = model.state_dict()\n","        torch.save(self.best_weights, self.path_to_save)\n","        # Update the stored best loss (needed if comparing score < best_score + delta)\n","        # self.best_val_loss = val_loss # Already updated before calling save_checkpoint\n","\n","    def load_best_weights(self, model):\n","         if self.restore_best_weights and self.best_weights is not None:\n","             print(f\"Restoring model weights from epoch {self.best_epoch} with val_loss: {self.best_val_loss:.4f}\")\n","             model.load_state_dict(self.best_weights)\n","         elif self.restore_best_weights:\n","              print(\"Warning: restore_best_weights=True but no best weights were saved.\")"]},{"cell_type":"code","execution_count":18,"id":"d4e39f20","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:46.820518Z","iopub.status.busy":"2025-05-25T15:35:46.819914Z","iopub.status.idle":"2025-05-25T15:35:46.83018Z","shell.execute_reply":"2025-05-25T15:35:46.829591Z"},"papermill":{"duration":0.025203,"end_time":"2025-05-25T15:35:46.831342","exception":false,"start_time":"2025-05-25T15:35:46.806139","status":"completed"},"tags":[]},"outputs":[],"source":["def train_and_eval_model(model, train_loader, test_loader, criterion, optimizer, device, epochs=100, scheduler=None, early_stopping=None):\n","\n","    if model is None or train_loader is None or test_loader is None:\n","        print(\"Model or DataLoaders not initialized. Skipping training.\")\n","        return [], []\n","\n","    train_losses = []\n","    test_losses = []\n","\n","    print(\"\\\\n--- Starting Training ---\")\n","    for epoch in range(epochs):\n","        model.train()\n","        running_train_loss = 0.0\n","        # Use tqdm for progress bar\n","        train_pbar = tqdm(train_loader, unit='batch', desc=f'Train Epoch {epoch + 1}/{epochs}')\n","        for X_batch, y_batch in train_pbar:\n","            # Move data to the correct device\n","            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","\n","            # Forward pass\n","            ypred = model(X_batch)\n","            loss = criterion(ypred, y_batch)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            # Optional: Gradient Clipping\n","            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","            optimizer.step()\n","\n","            running_train_loss += loss.item() * X_batch.size(0) # Weighted average contribution\n","            train_pbar.set_postfix(loss=loss.item()) # Show loss in progress bar\n","\n","\n","        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_train_loss)\n","        print(f'---> Epoch {epoch + 1} Train Loss: {epoch_train_loss:.4f}')\n","\n","        # Validation phase\n","        model.eval()\n","        running_test_loss = 0.0\n","        test_pbar = tqdm(test_loader, unit='batch', desc=f'Eval Epoch {epoch + 1}/{epochs}', leave=False)\n","        with torch.no_grad(): # Use no_grad for evaluation efficiency\n","            for X_batch, y_batch in test_pbar:\n","                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n","                ypred = model(X_batch)\n","                loss = criterion(ypred, y_batch)\n","                running_test_loss += loss.item() * X_batch.size(0)\n","                test_pbar.set_postfix(loss=loss.item())\n","\n","        epoch_test_loss = running_test_loss / len(test_loader.dataset)\n","        test_losses.append(epoch_test_loss)\n","        print(f'---> Epoch {epoch + 1} Test Loss:  {epoch_test_loss:.4f}')\n","\n","        # Learning rate scheduling step\n","        if scheduler:\n","            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","                scheduler.step(epoch_test_loss) # Pass validation loss\n","                # Print current LR (optional)\n","                # current_lr = optimizer.param_groups[0]['lr']\n","                # print(f\"    Current LR: {current_lr:.6f}\")\n","            else:\n","                 scheduler.step() # For other schedulers like StepLR\n","\n","        # Early stopping check\n","        if early_stopping:\n","            if early_stopping(epoch_test_loss, epoch + 1, model): # Pass epoch number (1-based)\n","                break # Stop training loop\n","\n","        print(\"-\" * 50) # Separator between epochs\n","\n","    print(\"--- Training Finished ---\")\n","\n","    # Load best weights if early stopping was used and configured to restore\n","    if early_stopping:\n","        early_stopping.load_best_weights(model)\n","\n","\n","    return train_losses, test_losses"]},{"cell_type":"code","execution_count":19,"id":"3a00456d","metadata":{"execution":{"iopub.execute_input":"2025-05-25T15:35:46.917887Z","iopub.status.busy":"2025-05-25T15:35:46.917355Z","iopub.status.idle":"2025-05-25T16:20:33.746762Z","shell.execute_reply":"2025-05-25T16:20:33.746052Z"},"papermill":{"duration":2686.84465,"end_time":"2025-05-25T16:20:33.748011","exception":false,"start_time":"2025-05-25T15:35:46.903361","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["\\n--- Starting Training ---\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 1/10: 100%|██████████| 7112/7112 [04:17<00:00, 27.61batch/s, loss=16.1]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 1 Train Loss: 17.9996\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                 \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 1 Test Loss:  15.1451\n","Validation loss decreased (15.1451 --> 15.1451). Saving model to best_model_cnnlstm_corrected.pth ...\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 2/10: 100%|██████████| 7112/7112 [04:13<00:00, 28.08batch/s, loss=14.3]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 2 Train Loss: 15.4722\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 2 Test Loss:  15.1490\n","EarlyStopping counter: 1 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 3/10: 100%|██████████| 7112/7112 [04:09<00:00, 28.55batch/s, loss=15.2]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 3 Train Loss: 15.4481\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 3 Test Loss:  15.1542\n","EarlyStopping counter: 2 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 4/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.60batch/s, loss=9.37]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 4 Train Loss: 15.4410\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 4 Test Loss:  15.1439\n","EarlyStopping counter: 3 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 5/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.61batch/s, loss=17.2]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 5 Train Loss: 15.4693\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 5 Test Loss:  15.1578\n","EarlyStopping counter: 4 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 6/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.58batch/s, loss=17.7]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 6 Train Loss: 15.4549\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 6 Test Loss:  15.1564\n","EarlyStopping counter: 5 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 7/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.61batch/s, loss=15.3]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 7 Train Loss: 15.4274\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 7 Test Loss:  15.1477\n","EarlyStopping counter: 6 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 8/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.59batch/s, loss=17.7]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 8 Train Loss: 15.4331\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 8 Test Loss:  15.1495\n","EarlyStopping counter: 7 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 9/10: 100%|██████████| 7112/7112 [04:08<00:00, 28.58batch/s, loss=20.4]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 9 Train Loss: 15.4400\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                   \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 9 Test Loss:  15.1492\n","EarlyStopping counter: 8 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Train Epoch 10/10: 100%|██████████| 7112/7112 [04:09<00:00, 28.54batch/s, loss=15.5]\n"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 10 Train Loss: 15.4323\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                    \r"]},{"name":"stdout","output_type":"stream","text":["---> Epoch 10 Test Loss:  15.1494\n","EarlyStopping counter: 9 out of 50 (Best val_loss: 15.1451 at epoch 1)\n","--------------------------------------------------\n","--- Training Finished ---\n","Restoring model weights from epoch 1 with val_loss: 15.1451\n","\\n--- Post-Training Evaluation ---\n","Minimum Training Loss: 15.4274\n","Minimum Validation Loss: 15.1439\n"]}],"source":["# Ensure model and dataloaders are ready\n","if model and train_loader and test_loader and criterion and optimizer and device:\n","    # Initialize Early Stopping\n","    early_stopper = EarlyStopping(path_to_save='best_model_cnnlstm_corrected.pth', patience=50, delta=0.01, verbose=True) # Increased patience\n","\n","    # --- Start Training ---\n","    num_epochs = 10 # Adjust as needed\n","    train_losses, test_losses = train_and_eval_model(\n","        model=model,\n","        train_loader=train_loader,\n","        test_loader=test_loader,\n","        criterion=criterion,\n","        optimizer=optimizer,\n","        device=device, # Pass the device\n","        epochs=num_epochs,\n","        scheduler=scheduler,\n","        early_stopping=early_stopper\n","    )\n","\n","    print(\"\\\\n--- Post-Training Evaluation ---\")\n","    print(f\"Minimum Training Loss: {np.min(train_losses):.4f}\")\n","    print(f\"Minimum Validation Loss: {np.min(test_losses):.4f}\")\n","    if early_stopper.early_stop:\n","         print(f\"Stopped early at epoch {early_stopper.best_epoch}. Best Validation Loss: {early_stopper.best_val_loss:.4f}\")\n","\n","else:\n","    print(\"\\\\n--- Cannot Start Training due to Initialization Errors ---\")\n","    train_losses, test_losses = [], [] # Assign empty lists"]},{"cell_type":"code","execution_count":20,"id":"0385d685","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:20:44.798515Z","iopub.status.busy":"2025-05-25T16:20:44.797782Z","iopub.status.idle":"2025-05-25T16:20:45.009104Z","shell.execute_reply":"2025-05-25T16:20:45.008331Z"},"papermill":{"duration":5.662311,"end_time":"2025-05-25T16:20:45.010444","exception":false,"start_time":"2025-05-25T16:20:39.348133","status":"completed"},"tags":[]},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","#del model"]},{"cell_type":"code","execution_count":21,"id":"73e09de2","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:20:55.889934Z","iopub.status.busy":"2025-05-25T16:20:55.889309Z","iopub.status.idle":"2025-05-25T16:20:55.89272Z","shell.execute_reply":"2025-05-25T16:20:55.89204Z"},"papermill":{"duration":5.482247,"end_time":"2025-05-25T16:20:55.894009","exception":false,"start_time":"2025-05-25T16:20:50.411762","status":"completed"},"tags":[]},"outputs":[],"source":["# if train_losses and test_losses:\n","#     print(f\"Final Minimum Training Loss: {np.min(train_losses):.4f}\")\n","#     print(f\"Final Minimum Validation Loss: {np.min(test_losses):.4f}\")\n","# else:\n","#      print(\"No training results available.\")"]},{"cell_type":"code","execution_count":22,"id":"29514b1f","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:21:06.725Z","iopub.status.busy":"2025-05-25T16:21:06.72465Z","iopub.status.idle":"2025-05-25T16:21:07.093813Z","shell.execute_reply":"2025-05-25T16:21:07.093116Z"},"papermill":{"duration":5.766874,"end_time":"2025-05-25T16:21:07.095008","exception":false,"start_time":"2025-05-25T16:21:01.328134","status":"completed"},"tags":[]},"outputs":[{"data":{"image/png":"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\n","text/plain":["<Figure size 1000x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["if train_losses and test_losses:\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(test_losses, label='Validation Loss')\n","\n","    # Find the epoch with the minimum validation loss\n","    min_val_loss_epoch = np.argmin(test_losses)\n","    min_val_loss = np.min(test_losses)\n","    plt.scatter(min_val_loss_epoch, min_val_loss, color='red', zorder=5, label=f'Best Val Loss: {min_val_loss:.4f}')\n","\n","    # If early stopping happened, mark where it stopped conceptually (best epoch)\n","    if 'early_stopper' in globals() and early_stopper.early_stop:\n","         plt.axvline(early_stopper.best_epoch -1 , color='r', linestyle='--', label=f'Best Epoch: {early_stopper.best_epoch}') # epoch is 1-based\n","\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss (L1 MAE)')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","else:\n","    print(\"No training data to plot.\")"]},{"cell_type":"code","execution_count":23,"id":"c33de543","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:21:18.21283Z","iopub.status.busy":"2025-05-25T16:21:18.212534Z","iopub.status.idle":"2025-05-25T16:21:19.368363Z","shell.execute_reply":"2025-05-25T16:21:19.367488Z"},"papermill":{"duration":6.603422,"end_time":"2025-05-25T16:21:19.369774","exception":false,"start_time":"2025-05-25T16:21:12.766352","status":"completed"},"tags":[]},"outputs":[],"source":["np.save(\"X_train_corrected.npy\", X_train.cpu().numpy() if 'X_train' in globals() else None)"]},{"cell_type":"code","execution_count":24,"id":"996061e8","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:21:30.814162Z","iopub.status.busy":"2025-05-25T16:21:30.813641Z","iopub.status.idle":"2025-05-25T16:21:31.343688Z","shell.execute_reply":"2025-05-25T16:21:31.342861Z"},"papermill":{"duration":6.121536,"end_time":"2025-05-25T16:21:31.345054","exception":false,"start_time":"2025-05-25T16:21:25.223518","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<Info | 11 non-empty values\n"," bads: []\n"," ch_names: parcel_0, parcel_1, parcel_2, parcel_3, parcel_4, parcel_5, ...\n"," chs: 52 misc, 17 Stimulus\n"," custom_ref_applied: False\n"," description: Anonymized using a time shift to preserve age at acquisition OSL ...\n"," dig: 0 items\n"," file_id: 4 items (dict)\n"," highpass: 0.0 Hz\n"," lowpass: 125.0 Hz\n"," meas_date: 1916-01-09 10:21:05 UTC\n"," meas_id: 4 items (dict)\n"," nchan: 69\n"," projs: []\n"," sfreq: 250.0 Hz\n",">\n"]}],"source":["import mne\n","# Define the path to the file\n","file_path = \"/kaggle/input/eeg-brain/Neural-Science/data/group_1/sub-CC110033_sflip_parc-raw.fif\"\n","# Load the raw data\n","raw = mne.io.read_raw_fif(file_path, preload=True)\n","# Display information about the raw object\n","print(raw.info)"]},{"cell_type":"code","execution_count":25,"id":"05f2c587","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:21:42.357248Z","iopub.status.busy":"2025-05-25T16:21:42.356964Z","iopub.status.idle":"2025-05-25T16:21:42.418195Z","shell.execute_reply":"2025-05-25T16:21:42.417421Z"},"papermill":{"duration":5.727485,"end_time":"2025-05-25T16:21:42.419393","exception":false,"start_time":"2025-05-25T16:21:36.691908","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["<Info | 11 non-empty values\n"," bads: []\n"," ch_names: parcel_0, parcel_1, parcel_2, parcel_3, parcel_4, parcel_5, ...\n"," chs: 52 misc\n"," custom_ref_applied: False\n"," description: Anonymized using a time shift to preserve age at acquisition OSL ...\n"," dig: 0 items\n"," file_id: 4 items (dict)\n"," highpass: 0.0 Hz\n"," lowpass: 125.0 Hz\n"," meas_date: 1916-01-09 10:21:05 UTC\n"," meas_id: 4 items (dict)\n"," nchan: 52\n"," projs: []\n"," sfreq: 250.0 Hz\n",">\n"]}],"source":["# Extract only MISC channels\n","misc_channels = raw.copy().pick_types(misc=True)\n","# Display information about the extracted channels\n","print(misc_channels.info)"]},{"cell_type":"code","execution_count":26,"id":"ee20458d","metadata":{"execution":{"iopub.execute_input":"2025-05-25T16:21:53.222901Z","iopub.status.busy":"2025-05-25T16:21:53.222571Z","iopub.status.idle":"2025-05-25T16:21:53.227265Z","shell.execute_reply":"2025-05-25T16:21:53.226641Z"},"papermill":{"duration":5.432621,"end_time":"2025-05-25T16:21:53.228296","exception":false,"start_time":"2025-05-25T16:21:47.795675","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Total recording time: 574.00 seconds\n"]}],"source":["# Get the total recording time in seconds\n","total_duration = raw.times[-1]\n","# Alternatively, using the raw.info dictionary\n","total_duration_alt = raw.n_times / raw.info['sfreq']\n","# Print the total duration\n","print(f\"Total recording time: {total_duration:.2f} seconds\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7153305,"sourceId":11421984,"sourceType":"datasetVersion"}],"dockerImageVersionId":31011,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"papermill":{"default_parameters":{},"duration":2987.582626,"end_time":"2025-05-25T16:22:02.06428","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-05-25T15:32:14.481654","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}